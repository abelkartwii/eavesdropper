version: "3.9"
services:
  # kafka image deprecated, use https://hub.docker.com/r/confluentinc/cp-kafka instead?
  # maybe check out: https://github.com/ksindi/kafka-compose (composes for kafka stacks)
  kafka:
    image: wurstmeister/kafka:2.13-2.7.0
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092"
    environment:
      - DOCKER_API_VERSION=1.22
      - KAFKA_ADVERTISED_HOST_NAME=127.0.0.1
      - KAFKA_ADVERTISED_PORT=9092
      - KAFKA_MESSAGE_MAX_BYTES=200000
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181

      # 1 partition, 1 replica, cleanup.policy set to compact
      - KAFKA_CREATE_TOPICS="eavesdropper:1:1:compact" 
    healthcheck:
      # from https://github.com/eciavatta/smack-template/blob/develop/docker-compose.yml
      - test: ["CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --zookeeper $$KAFKA_ZOOKEEPER_CONNECT --list"]
      - interval: 60s
      - timeout: 15s
      - retries: 3
    depends_on:
      - zookeeper
    networks:
      - eavesdropper
  
  kafka-producer:
    image: python:3-alpine 
    container_name: kafka-producer
    command: >
      sh -c "pip install -r /usr/src/producer/requirements.txt
      && python3 /usr/src/producer/kafkaProducer.py"
    volumes:
      - ./kafka:/usr/src/producer
    networks:
      - eavesdropper

  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    hostname: zookeeper
    ports:
      - "2181:2181"
    healthcheck:
      - test: echo stat | nc localhost 2181
      - interval: 15s
      - timeout: 10s
      - retries: 3
    networks:
      - eavesdropper

  # using this man's example https://github.com/antlypls/spark-demos/blob/master/docker-spark-streaming-sql/docker-compose.yml
  java:
    image: openjdk:jre
    command: bash
    ports:
      - "4040:4040"
    volumes:
      - ./build:/build
    working_dir: /build
    depends_on: 
      - zookeeper
      - kafka

      # hmm https://www.baeldung.com/ops/kafka-docker-setup
  
  # spark: https://github.com/big-data-europe/docker-spark/blob/master/docker-compose.yml
  # also this https://github.com/vindeolal/spark-cassandra-kafka-env/blob/master/docker-compose.yml
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    hostname: spark-master
    ports: 
      - "8080:8080"
      - "7077:7077"
      - "6066:6066"
    networks:
      - eavesdropper
    environment:
      - INIT_DAEMON_STEP: setup_spark

  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker1
    volumes: 
      - ./spark:/sparkConsumer
    ports:
      - "8081:8081"
    networks:
      - eavesdropper
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"

  # ummm or https://github.com/ksindi/kafka-compose/blob/master/kafka-pyspark-demo/docker-compose.yml
  spark:
    image: jupyter/pyspark-notebook
    network_mode: host
    links:
     - kafka
    ports:
      - "18888:8888"

  # ikutin yg atas ada healthcheck juga
  cassandra:
    image: cassandra:3
    # enable volume to persist data outside of containers
    volumes:
      - ./datadir:/var/lib/cassandra
    healthcheck:
      - test: ["CMD-SHELL", "[ $$(nodetool statusgossip) = running ]"]
      - interval: 15s
      - timeout: 10s
      - retries: 3
    ports:
      - "9042:9042"

networks:
  eavesdropper:
    driver: bridge

  # metabase:
  #       image: metabase/metabase
  #       ports:
  #           - "3000:3000"

  #   postgres:
  #       image: postgres:9.6
  #       environment:
  #           - POSTGRES_USER=airflow
  #           - POSTGRES_PASSWORD=airflow
  #           - POSTGRES_DB=airflow
  #       logging:
  #           options:
  #               max-size: 10m
  #               max-file: "3"

  #   webserver:
  #       image: puckel/docker-airflow:1.10.9
  #       restart: always
  #       depends_on:
  #           - postgres
  #       environment:
  #           - LOAD_EX=n
  #           - EXECUTOR=Local
  #       logging:
  #           options:
  #               max-size: 10m
  #               max-file: "3"
  #       volumes:
  #           - ./airflow/dags:/usr/local/airflow/dags
  #           - ./airflow/plugins:/usr/local/airflow/plugins
  #           - ./requirements.txt:/requirements.txt
  #       ports:
  #           - "8080:8080"
  #       command: webserver
  #       healthcheck:
  #           test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
  #           interval: 30s
  #           timeout: 30s
  #           retries: 3
