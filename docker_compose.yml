version: "3.9"
services:
  kafka:
    image: wurstmeister/kafka:2.13-2.7.0
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092"
    environment:
      - DOCKER_API_VERSION: 1.22
      - KAFKA_ADVERTISED_HOST_NAME: 127.0.0.1
      - KAFKA_ADVERTISED_PORT: 9092
      - KAFKA_CREATE_TOPICS: "eavesdropper:1:1:compact"
    networks:
      - eavesdropper
  
  kafka-producer:
    image: python:3-alpine 
    container_name: kafka-producer
    command: >
      sh -c "pip install -r /usr/src/producer/requirements.txt
      && python3 /usr/src/producer/kafkaProducerService.py"
    volumes:
      - ./kafkaProducer:/usr/src/producer
    networks:
      - eavesdropper

  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    hostname: zookeeper
    ports:
      - "2181:2181"
    networks:
      - eavesdropper

      # hmm https://www.baeldung.com/ops/kafka-docker-setup
  
  # spark: https://github.com/big-data-europe/docker-spark/blob/master/docker-compose.yml
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    hostname: spark-master
    ports: 
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark

  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    networks:
      - eavesdropper
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"

networks:
  eavesdropper:
    driver: bridge

  # metabase:
  #       image: metabase/metabase
  #       ports:
  #           - "3000:3000"

  #   postgres:
  #       image: postgres:9.6
  #       environment:
  #           - POSTGRES_USER=airflow
  #           - POSTGRES_PASSWORD=airflow
  #           - POSTGRES_DB=airflow
  #       logging:
  #           options:
  #               max-size: 10m
  #               max-file: "3"

  #   webserver:
  #       image: puckel/docker-airflow:1.10.9
  #       restart: always
  #       depends_on:
  #           - postgres
  #       environment:
  #           - LOAD_EX=n
  #           - EXECUTOR=Local
  #       logging:
  #           options:
  #               max-size: 10m
  #               max-file: "3"
  #       volumes:
  #           - ./airflow/dags:/usr/local/airflow/dags
  #           - ./airflow/plugins:/usr/local/airflow/plugins
  #           - ./requirements.txt:/requirements.txt
  #       ports:
  #           - "8080:8080"
  #       command: webserver
  #       healthcheck:
  #           test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
  #           interval: 30s
  #           timeout: 30s
  #           retries: 3
